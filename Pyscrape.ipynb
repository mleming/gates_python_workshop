{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyscrape\n",
    "\n",
    "_A simple web crawler in Python_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "This workshop assumes you are familiar with the concepts of:\n",
    "\n",
    "* variables,\n",
    "* strings,\n",
    "* conditions,\n",
    "* loops,\n",
    "* functions.\n",
    "\n",
    "### Topics in this workshop\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "* import a Python library and use its documentation,\n",
    "* use regular expressions to search for patterns in data,\n",
    "* use exceptions for error handling,\n",
    "* recognise a tree data structure,\n",
    "* use recursion to traverse a tree structure,\n",
    "* use stacks to traverse a tree structure,\n",
    "* tell the difference between depth-first search and breadth-first search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0: Library import\n",
    "\n",
    "The greatest strength of Python comes from its large collection of libraries. For this workshop, we will be using the `urllib2` library for loading web pages, and the `re` library for matching patterns in strings.\n",
    "\n",
    "Libraries in Python are loaded using the keyword\n",
    "\n",
    "```Python\n",
    "import libraryname\n",
    "```\n",
    "\n",
    "The documentation for the two libraries can be found at:\n",
    "\n",
    "https://docs.python.org/2/library/urllib2.html\n",
    "\n",
    "https://docs.python.org/2/library/re.html\n",
    "\n",
    "To access a function `f` from library `l`, use the syntax:\n",
    "\n",
    "```Python\n",
    "f.l()\n",
    "```\n",
    "\n",
    "Use the cell below to import the two libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Find all URLs in a webpage\n",
    "\n",
    "### Task 1.1: Opening a webpage\n",
    "\n",
    "The `urrlib2` library contains a function called `urlopen`. Click on the following link to see the documentation for this function:\n",
    "\n",
    "https://docs.python.org/2/library/urllib2.html\n",
    "\n",
    "We will only use the first argument of this function: `url`, passed as a string. The function sends a request to the webpage, and returns the contents of the website. To get the contents as a string, call the `read` function with no arguments on the returned object. You can use the `print` keyword to print the contents of a string, such as:\n",
    "\n",
    "```Python\n",
    "print \"Hello World\"\n",
    "```\n",
    "\n",
    "Websites are usually large documents. If you want to try loading a website and printing its contents, use the following url:\n",
    "\n",
    "http://www.lorem-ipsum-text.com\n",
    "\n",
    "Use the cell below to load a website into a variable. Call `read` on this variable to get the contents of the website, and use the `print` keyword to print its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://www.lorem-ipsum-text.com\"\n",
    "data = urllib2.urlopen(url)\n",
    "webpage = data.read()\n",
    "print webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to clear the printed text, select the cell above and go to\n",
    "\n",
    "Cell > Current Outputs > Clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Regular expressions\n",
    "\n",
    "Regular expressions are a compact way of representing patterns in strings. We will use pattern matching to look for the pattern of a URL on a website. All URLs we want to find are of the form:\n",
    "\n",
    "`http://webpage.domain/` or\n",
    "\n",
    "`https://webpage.domain/`.\n",
    "\n",
    "The pattern we want to search for is a string starting with `http` or `https`, followed by `://` and followed by a string of uppercase or lowercase letters and dots, and ending with `/`.\n",
    "\n",
    "The function `findall` of the `re` library searches for patterns in a string. Click the following link to see its documentation:\n",
    "\n",
    "https://docs.python.org/2/library/re.html\n",
    "\n",
    "The function accepts a pattern, specified as a regular expression, and a string in which to search. It returns a list of strings which matched the pattern.\n",
    "\n",
    "#### A brief introduction to regular expression patterns\n",
    "\n",
    "You will need to construct a pattern which matches the strings described above. The following special characters may be helpful:\n",
    "\n",
    "| Special character | Description | Example | Return value |\n",
    "|-------------------|-------------|---------|--------------|\n",
    "| Normal text       | Matches only the text itself. | `re.findall('abc', 'abcdef')` | `['abc']` |\n",
    "| `?`               | Makes the preceding character optional. | `re.findall('a?', 'abcdefa')` | `['ab', 'a']` |\n",
    "| `*`               | Matches any number of occurrences of the preceding character. | `re.findall('a*', 'abcdaabcaaa')` | `['a', 'aa', 'aaa']` |\n",
    "| `[]`              | Matches the group of characters within the brackets. | `re.findall('[123]', '42f1A')` | `['2', '1']` |\n",
    "| `-`               | Use in a group to match a range of characters. | `re.findall('[a-z1]', '42f1A')` | `['2', 'f', '1']` |\n",
    "\n",
    "Since some special characters are reserved for describing patterns, they cannot be matched directly. In this case, type `\\` before the character and it will be interpreted literally and not as a pattern description. This is called __escaping__. Examples of escaped characters include `.`, `/` and `*`. \n",
    "\n",
    "Use the cell below to construct a regular expression which matches websites from the string in `urls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = \"\"\"\n",
    "Hello world, visit https://www.gatescambridge.org/ to learn about Gates Cambridge.\n",
    "Information about University of Cambridge can be found at http://www.cam.ac.uk/.\n",
    "Go to https://docs.python.org/2/library/re.html for Python documentation.\n",
    "\"\"\"\n",
    "\n",
    "# Construct your regular expression here:\n",
    "regex = 'https?:\\/\\/[\\.a-zA-Z]*\\/'\n",
    "\n",
    "# Print all websites from the string in urls\n",
    "print re.findall(regex, urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Search for ULRs in a webpage\n",
    "\n",
    "Use the script for loading a webpage and the regular expression for finding URLs to find all URLs in a webpage of your choosing. For example, you may use http://google.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://google.com\"\n",
    "data = urllib2.urlopen(url)\n",
    "webpage = data.read()\n",
    "m = re.findall('https?:\\/\\/[^\\/]*',webpage)\n",
    "for address in m:\n",
    "    print address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create a reusable function\n",
    "\n",
    "### Task 2.1: Exceptions\n",
    "\n",
    "_let them try running the program with a broken website and see what happens_\n",
    "\n",
    "_explanation of the try and except keywords_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://google.com\"\n",
    "try:\n",
    "    data = urllib2.urlopen(url)\n",
    "    webpage = data.read()\n",
    "    m = re.findall('https?:\\/\\/[^\\/]*',webpage)\n",
    "    for address in m:\n",
    "        print address\n",
    "except:\n",
    "    print \"Error \" + url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Pyscrape function\n",
    "\n",
    "_explain functions (briefly because prerequisite)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pyscrape(url):\n",
    "    try:\n",
    "        data = urllib2.urlopen(url)\n",
    "        webpage = data.read()\n",
    "    except:\n",
    "        print \"Error: \" + url\n",
    "        return []\n",
    "    return re.findall('https?:\\/\\/[\\.a-zA-Z]*\\/', webpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try calling the `pyscrape` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://schema.org/',\n",
       " 'http://www.google.com/',\n",
       " 'http://www.google.co.uk/',\n",
       " 'http://maps.google.co.uk/',\n",
       " 'https://play.google.com/',\n",
       " 'http://www.youtube.com/',\n",
       " 'http://news.google.co.uk/',\n",
       " 'https://mail.google.com/',\n",
       " 'https://drive.google.com/',\n",
       " 'https://www.google.co.uk/',\n",
       " 'http://www.google.co.uk/',\n",
       " 'https://accounts.google.com/',\n",
       " 'http://www.google.co.uk/',\n",
       " 'https://plus.google.com/',\n",
       " 'http://www.google.co.uk/']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyscrape(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Recursion\n",
    "\n",
    "### Task 3.1: Write a recursive URL scraper\n",
    "\n",
    "_explain recursion - function calling itself_\n",
    "\n",
    "_explain optional arguments_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEPTH_LIMIT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pyscrape_recurse(url, depth = 0):\n",
    "    if depth > DEPTH_LIMIT:\n",
    "        return\n",
    "    print '    ' * depth + url\n",
    "    urlsInWebpage = pyscrape(url)\n",
    "    for u in urlsInWebpage:\n",
    "        pyscrape_recurse(u, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyscrape_recurse(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Remove duplicate URLs\n",
    "\n",
    "_remember URLs we've seen before and don't visit them again_\n",
    "\n",
    "_explain sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visited = set()\n",
    "\n",
    "def pyscrape_unique_recurse(url, depth = 0):\n",
    "    if depth > DEPTH_LIMIT:\n",
    "        return\n",
    "    print '    ' * depth + url\n",
    "    urlsInWebpage = pyscrape(url)\n",
    "    for u in urlsInWebpage:\n",
    "        if u not in visited:\n",
    "            visited.add(u)\n",
    "            pyscrape_unique_recurse(url, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://google.com\n",
      "    http://google.com\n",
      "        http://google.com\n"
     ]
    }
   ],
   "source": [
    "pyscrape_unique_recurse(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://schema.org\n",
      "http://www.google.com\n",
      "http://www.google.co.uk\n",
      "http://maps.google.co.uk\n",
      "https://play.google.com\n",
      "http://www.youtube.com\n",
      "http://news.google.co.uk\n",
      "https://mail.google.com\n",
      "https://drive.google.com\n",
      "https://www.google.co.uk\n",
      "http://www.google.co.uk\n",
      "https://accounts.google.com\n",
      "http://www.google.co.uk\n",
      "https://plus.google.com\n",
      "http://www.google.co.uk\n",
      "https://google.com/\n",
      "https://www.youtube.com/\n",
      "https://s.ytimg.com/\n",
      "Error: https://s.ytimg.com/\n",
      "https://www.google.co.uk/\n",
      "http://www.google.com/\n",
      "http://schema.org/\n",
      "http://github.com/\n",
      "https://collector.githubapp.com/\n",
      "http://ogp.me/\n",
      "http://openwebfoundation.org/\n",
      "http://www.gstatic.com/\n",
      "Error: http://www.gstatic.com/\n",
      "http://microformats.org/\n",
      "http://mediatemple.net/\n",
      "Error: http://mediatemple.net/\n",
      "http://www.readwriteweb.com/\n",
      "http://www.criticalvc.com/\n",
      "https://stats.wp.com/\n",
      "https://connect.facebook.net/\n",
      "Error: https://connect.facebook.net/\n",
      "https://snap.licdn.com/\n",
      "https://pages.awscloud.com/\n",
      "https://youtu.be/\n",
      "https://quicksight.aws/\n",
      "https://www.matillion.com/\n",
      "https://t.co/\n",
      "https://abs.twimg.com/\n",
      "Error: https://abs.twimg.com/\n",
      "https://yoast.com/\n",
      "https://www.googletagmanager.com/\n",
      "https://www.thinkwithgoogle.com/\n",
      "http://feeds.feedburner.com/\n",
      "Error: http://feeds.feedburner.com/\n",
      "https://analyticsacademy.withgoogle.com/\n",
      "https://www.en.advertisercommunity.com/\n",
      "https://google.qualtrics.com/\n",
      "https://googleen.i.lithium.com/\n",
      "https://analytics.googleblog.com/\n",
      "http://google.starttest.com/\n",
      "https://resources.blogblog.com/\n",
      "Error: https://resources.blogblog.com/\n",
      "http://www.gabreakthrough.com/\n",
      "http://www.leapica.com/\n",
      "https://leapica.leadpages.co/\n",
      "https://www.getdrip.com/\n",
      "https://drip.getambassador.com/\n",
      "http://toolbar.conduit.com/\n"
     ]
    }
   ],
   "source": [
    "# PART 4 - BFS and DFS and other fancy regex stuff.\n",
    "# This program uses a search to keep following links on webpages. It does more\n",
    "# complicated things, like not following links to previously-visited websites,\n",
    "# which means that this whole thing will definitely fill up two hours.\n",
    "def printscrape(breadth, top_url):\n",
    "\t# If breadth == True, this performs a breadth-first search.\n",
    "\t# If breadth == False, this performs a depth-first search.\n",
    "\tpop_end = -1 if breadth else 0\n",
    "\tstack = []\n",
    "\tstack.append(top_url)\n",
    "\tfrontier = []\n",
    "\ti = 0\n",
    "\twhile len(stack) > 0:\n",
    "\t\ti += 1\n",
    "\t\tif i == 100: # Stops it from running forever.\n",
    "\t\t\tprint frontier\n",
    "\t\t\treturn\n",
    "\t\tfoo = stack[pop_end]\n",
    "\t\tprint stack[pop_end]\n",
    "\t\tdel stack[pop_end]\n",
    "\t\tfor d in pyscrape(foo):\n",
    "\t\t\ttry:\n",
    "\t\t\t# This regular expression will match the \".google.com/\"\n",
    "\t\t\t# in \"https://www.google.com/\". It's used in this context to\n",
    "\t\t\t# make sure that we're calling a unique website each time instead of\n",
    "\t\t\t# different parts of the same website (so, if we access\n",
    "\t\t\t# \"https://www.google.com/\", the search will ignore\n",
    "\t\t\t# \"https://news.google.com/\"\n",
    "\t\t\t\tmm = re.search('[a-zA-Z]+\\.[a-zA-Z]+\\/$',d).group(0)\n",
    "\t\t\t\tif not(any(mm in f for f in frontier)):\n",
    "\t\t\t\t\tfrontier.append(d)\n",
    "\t\t\t\t\tstack.append(d)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint d\n",
    "\n",
    "printscrape(True,\"https://google.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
