{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyscrape\n",
    "\n",
    "_A simple web crawler in Python_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "This workshop assumes you are familiar with the concepts of:\n",
    "\n",
    "* variables,\n",
    "* strings,\n",
    "* conditions,\n",
    "* loops,\n",
    "* functions.\n",
    "\n",
    "### Topics in this workshop\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "* import a Python library and use its documentation,\n",
    "* use regular expressions to search for patterns in data,\n",
    "* use exceptions for error handling,\n",
    "* recognise a tree data structure,\n",
    "* use recursion to traverse a tree structure,\n",
    "* use stacks to traverse a tree structure,\n",
    "* tell the difference between depth-first search and breadth-first search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0: Library import\n",
    "\n",
    "The greatest strength of Python comes from its large collection of libraries. For this workshop, we will be using the `urllib2` library for loading web pages, and the `re` library for matching patterns in strings.\n",
    "\n",
    "Libraries in Python are loaded using the keyword\n",
    "\n",
    "```Python\n",
    "import libraryname\n",
    "```\n",
    "\n",
    "The documentation for the two libraries can be found at:\n",
    "\n",
    "https://docs.python.org/2/library/urllib2.html\n",
    "\n",
    "https://docs.python.org/2/library/re.html\n",
    "\n",
    "To access a function `f` from library `l`, use the syntax:\n",
    "\n",
    "```Python\n",
    "f.l()\n",
    "```\n",
    "\n",
    "Use the cell below to import the two libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Find all URLs in a webpage\n",
    "\n",
    "### Task 1.1: Opening a webpage\n",
    "\n",
    "The `urrlib2` library contains a function called `urlopen`. Click on the following link to see the documentation for this function:\n",
    "\n",
    "https://docs.python.org/2/library/urllib2.html\n",
    "\n",
    "We will only use the first argument of this function: `url`, passed as a string. The function sends a request to the webpage, and returns the contents of the website. To get the contents as a string, call the `read` function with no arguments on the returned object. You can use the `print` keyword to print the contents of a string, such as:\n",
    "\n",
    "```Python\n",
    "print \"Hello World\"\n",
    "```\n",
    "\n",
    "Websites are usually large documents. If you want to try loading a website and printing its contents, use the following url:\n",
    "\n",
    "http://www.lorem-ipsum-text.com\n",
    "\n",
    "Use the cell below to load a website into a variable. Call `read` on this variable to get the contents of the website, and use the `print` keyword to print its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://www.lorem-ipsum-text.com\"\n",
    "data = urllib2.urlopen(url)\n",
    "webpage = data.read()\n",
    "print webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to clear the printed text, select the cell above and go to\n",
    "\n",
    "Cell > Current Outputs > Clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Regular expressions\n",
    "\n",
    "Regular expressions are a compact way of representing patterns in strings. We will use pattern matching to look for the pattern of a URL on a website. All URLs we want to find are of the form:\n",
    "\n",
    "`http://webpage.domain/` or\n",
    "\n",
    "`https://webpage.domain/`.\n",
    "\n",
    "The pattern we want to search for is a string starting with `http` or `https`, followed by `://` and followed by a string of uppercase or lowercase letters and dots, and ending with `/`.\n",
    "\n",
    "The function `findall` of the `re` library searches for patterns in a string. Click the following link to see its documentation:\n",
    "\n",
    "https://docs.python.org/2/library/re.html\n",
    "\n",
    "The function accepts a pattern, specified as a regular expression, and a string in which to search. It returns a list of strings which matched the pattern.\n",
    "\n",
    "#### A brief introduction to regular expression patterns\n",
    "\n",
    "You will need to construct a pattern which matches the strings described above. The following special characters may be helpful:\n",
    "\n",
    "| Special character | Description | Example | Return value |\n",
    "|-------------------|-------------|---------|--------------|\n",
    "| Normal text       | Matches only the text itself. | `re.findall('abc', 'abcdef')` | `['abc']` |\n",
    "| `?`               | Makes the preceding character optional. | `re.findall('a?', 'abcdefa')` | `['ab', 'a']` |\n",
    "| `*`               | Matches any number of occurrences of the preceding character. | `re.findall('a*', 'abcdaabcaaa')` | `['a', 'aa', 'aaa']` |\n",
    "| `[]`              | Matches the group of characters within the brackets. | `re.findall('[123]', '42f1A')` | `['2', '1']` |\n",
    "| `-`               | Use in a group to match a range of characters. | `re.findall('[a-z1]', '42f1A')` | `['f', '1']` |\n",
    "\n",
    "Since some special characters are reserved for describing patterns, they cannot be matched directly. In this case, type `\\` before the character and it will be interpreted literally and not as a pattern description. This is called __escaping__. Examples of escaped characters include `.`, `/` and `*`. \n",
    "\n",
    "Use the cell below to construct a regular expression which matches websites from the string in `urls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = \"\"\"\n",
    "Hello world, visit https://www.gatescambridge.org/ to learn about Gates Cambridge.\n",
    "Information about University of Cambridge can be found at http://www.cam.ac.uk/.\n",
    "Go to https://docs.python.org/2/library/re.html for Python documentation.\n",
    "\"\"\"\n",
    "\n",
    "# Construct your regular expression here:\n",
    "regex = 'https?:\\/\\/[\\.a-zA-Z0-9]*\\/'\n",
    "\n",
    "# Print all websites from the string in urls\n",
    "print re.findall(regex, urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Search for ULRs in a webpage\n",
    "\n",
    "Use the script for loading a webpage and the regular expression for finding URLs to find all URLs in a webpage of your choosing. For example, you may use http://google.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://google.com\"\n",
    "data = urllib2.urlopen(url)\n",
    "webpage = data.read()\n",
    "m = re.findall('https?:\\/\\/[^\\/]*',webpage)\n",
    "for address in m:\n",
    "    print address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create a reusable function\n",
    "\n",
    "### Task 2.1: Exceptions\n",
    "\n",
    "Some library functions in Python may return an error. For example, if you run the function with an incorrect argument, the function will complain about it. In the cell below, try running the `urlopen` function with a website which does not exist, for example `http://google.c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://google.c\"\n",
    "urllib2.urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function complains about the incorrect input, and it shows you exactly where in your code the error occurred. We say that the function has __thrown an exception__. The program will not progress beyond the point where an exception was thrown.\n",
    "\n",
    "If you run the `urlopen` function with an argument that you know is correct, you do not have to worry about exceptions. This is rarely the case in real world. In our `pyscrape` program, we will open URLs found on websites, which we cannot guarantee are correct. We do not want the program to stop in this case.\n",
    "\n",
    "Python provides a mechanism to _try out_ a piece of code. If it throws an exception, you can provide an alternative that is executed instead. In some languages, this is called __catching an exception__. In Python, the following syntax is used to catch and handle an exception:\n",
    "\n",
    "```Python\n",
    "try:\n",
    "    function_which_throws_an_exception(incorrect_input)\n",
    "    # Code here will run only if an exception is not thrown\n",
    "except:\n",
    "    # Code which executes if an exception is thrown\n",
    "# Code here will run whether or not an exception was thrown\n",
    "```\n",
    "\n",
    "Indentation in Python is important. The lines following `try` must all be indented (press TAB once), `except` must have the same indentation as `try`, and all indented code after `except` is a part of the exception handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://google.com\"\n",
    "try:\n",
    "    data = urllib2.urlopen(url)\n",
    "    webpage = data.read()\n",
    "    m = re.findall('https?:\\/\\/[^\\/]*',webpage)\n",
    "    for address in m:\n",
    "        print address\n",
    "except:\n",
    "    print \"Error \" + url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Pyscrape function\n",
    "\n",
    "_explain functions (briefly because prerequisite)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pyscrape(url):\n",
    "    try:\n",
    "        data = urllib2.urlopen(url)\n",
    "        webpage = data.read()\n",
    "    except:\n",
    "        print \"Error: \" + url\n",
    "        return []\n",
    "    return re.findall('https?:\\/\\/[\\.a-zA-Z]*\\/', webpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try calling the `pyscrape` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyscrape(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Recursion\n",
    "\n",
    "### Task 3.1: Write a recursive URL scraper\n",
    "\n",
    "_explain recursion - function calling itself_\n",
    "\n",
    "_add fib as an explanation_\n",
    "\n",
    "_explain optional arguments_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEPTH_LIMIT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pyscrape_recurse(url, depth = 0):\n",
    "    if depth > DEPTH_LIMIT:\n",
    "        return\n",
    "    print '    ' * depth + url\n",
    "    urlsInWebpage = pyscrape(url)\n",
    "    for u in urlsInWebpage:\n",
    "        pyscrape_recurse(u, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyscrape_recurse(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Remove duplicate URLs\n",
    "\n",
    "_remember URLs we've seen before and don't visit them again_\n",
    "\n",
    "_explain sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pyscrape_unique_recurse(url, visited = set(), depth = 0):\n",
    "    if depth > DEPTH_LIMIT:\n",
    "        return\n",
    "    print '    ' * depth + url\n",
    "    urlsInWebpage = pyscrape(url)\n",
    "    for u in urlsInWebpage:\n",
    "        if u not in visited:\n",
    "            visited.add(u)\n",
    "            pyscrape_unique_recurse(u, visited, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyscrape_unique_recurse(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Breadth-first and depth-first search\n",
    "\n",
    "_explain trees - how is a website like a tree_\n",
    "\n",
    "_recursion performs depth-first search - rewrite the pyscrape_unique_recurse function to instead use lists as stacks_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stack = []\n",
    "print stack\n",
    "\n",
    "stack.append(\"A\")\n",
    "print \"Add A  \" + str(stack)\n",
    "\n",
    "stack.append(\"B\")\n",
    "print \"Add B  \" + str(stack)\n",
    "\n",
    "stack.pop()\n",
    "print \"Pop    \" + str(stack)\n",
    "\n",
    "stack.append(\"C\")\n",
    "print \"Add C  \" + str(stack)\n",
    "\n",
    "stack.pop()\n",
    "print \"Pop    \" + str(stack)\n",
    "\n",
    "stack.pop()\n",
    "print \"Pop    \" + str(stack)\n",
    "\n",
    "# Uncomment to see what happens if you try removing an element from an empty stack\n",
    "# stack.pop()\n",
    "# print \"Pop    \" + str(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Depth-first search using stacks\n",
    "\n",
    "_include while loop in fill-in-the-blanks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAGE_LIMIT = 10\n",
    "\n",
    "def pyscrape_dfs(top_url):\n",
    "    visited = set()\n",
    "    stack = [top_url]\n",
    "    i = 0\n",
    "    \n",
    "    while len(stack) > 0 and i < PAGE_LIMIT:\n",
    "        i += 1\n",
    "        url = stack.pop()\n",
    "        print url\n",
    "        urls_in_webpage = pyscrape(url)\n",
    "        for u in urls_in_webpage:\n",
    "            if u not in visited:\n",
    "                visited.add(u)\n",
    "                stack.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyscrape_dfs(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.3: Queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queue = []\n",
    "print queue\n",
    "\n",
    "queue.append(\"A\")\n",
    "print \"Add A  \" + str(queue)\n",
    "\n",
    "queue.append(\"B\")\n",
    "print \"Add B  \" + str(queue)\n",
    "\n",
    "queue.pop(0)\n",
    "print \"Pop    \" + str(queue)\n",
    "\n",
    "queue.append(\"C\")\n",
    "print \"Add C  \" + str(queue)\n",
    "\n",
    "queue.pop(0)\n",
    "print \"Pop    \" + str(queue)\n",
    "\n",
    "queue.pop(0)\n",
    "print \"Pop    \" + str(queue)\n",
    "\n",
    "# Uncomment to see what happens if you try removing an element from an empty queue\n",
    "# queue.pop(0)\n",
    "# print \"Pop    \" + str(queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.4: Breadth-first search using queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAGE_LIMIT = 10\n",
    "\n",
    "def pyscrape_bfs(top_url):\n",
    "    visited = set()\n",
    "    queue = [top_url]\n",
    "    i = 0\n",
    "    \n",
    "    while len(queue) > 0 and i < PAGE_LIMIT:\n",
    "        i += 1\n",
    "        url = queue.pop(0)\n",
    "        print url\n",
    "        urls_in_webpage = pyscrape(url)\n",
    "        for u in urls_in_webpage:\n",
    "            if u not in visited:\n",
    "                visited.add(u)\n",
    "                queue.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyscrape_bfs(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.5: Combining the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PAGE_LIMIT = 10\n",
    "\n",
    "def pyscrape_search(top_url, breadth = False):\n",
    "    visited = set()\n",
    "    to_visit = [top_url]\n",
    "    i = 0\n",
    "    \n",
    "    while len(to_visit) > 0 and i < PAGE_LIMIT:\n",
    "        i += 1\n",
    "        url = to_visit.pop(0) if breadth else to_visit.pop()\n",
    "        print url\n",
    "        urls_in_webpage = pyscrape(url)\n",
    "        for u in urls_in_webpage:\n",
    "            if u not in visited:\n",
    "                visited.add(u)\n",
    "                to_visit.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Breadth-first search:\"\n",
    "pyscrape_search(\"http://google.com\", True)\n",
    "\n",
    "print \"Depth-first search:\"\n",
    "pyscrape_search(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Exercises for the keen\n",
    "\n",
    "### Exercise 1: Indentation for search\n",
    "\n",
    "Currently you cannot see which link led to which website in the search version of the program. Write a function `pyscrape_search_indent` in which printed-out URLs are properly indented.\n",
    "\n",
    "Hints:\n",
    "* Python contains a data type called a __tuple__, denoted by `()`. Tuples can hold several values of different types and you can access the first or second value using `t[0]` and `t[1]`.\n",
    "* Lists can hold values of many different data types, but in a single list, each value must have the same type.\n",
    "\n",
    "### Exercise 2: Search up to a given depth\n",
    "\n",
    "The `pyscrape_search` function only finds the first `PAGE_LIMIT` pages. Modify it to instead search to a given depth, similarly to the `pyscrape_recurse` functions.\n",
    "\n",
    "Hints:\n",
    "* You will not need the index `i`.\n",
    "* Completing Exercise 1 will help with this exercise.\n",
    "\n",
    "### Exercise 3: Do not visit different parts of the same website\n",
    "\n",
    "Pyscrape currently visits different parts of the same website. For example, it thinks news.google.com and google.com are different websites. Modify it to only visit one of them but ignore the other. Pay attention to domains which have two parts, for example .co.uk or .ac.uk - you don't want to ignore example.co.uk if you've visited somepage.co.uk before!\n",
    "\n",
    "Hints:\n",
    "* The documentation for regular expressions in Python has many examples.\n",
    "* When working with regular expressions, it is useful to create a few example strings and try your expression on them before running the whole program - it is faster and you have control over the examples you choose.\n",
    "\n",
    "### Exercise 4: Indent the error messages\n",
    "\n",
    "Error messages from Pyscrape are not indented like the printed-out websites. Modify the `pyscrape` function to indent them.\n",
    "\n",
    "Hints:\n",
    "* You may need to add another argument to `pyscrape`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
